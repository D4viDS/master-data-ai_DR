{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.5"
    },
    "colab": {
      "name": "Crea_tu_primer_dataset.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xZTgMQM_SXd"
      },
      "source": [
        "# Creando tu primer dataset desde cero\n",
        "La intención de este notebook es extraer letras de imágenes de un libro antiguo de cocina con el objetivo de crear un dataset que podrá ser utilizado para entrenar algoritmos de machine learning.\n",
        "\n",
        "\n",
        "Lo primero que haremos, será importar algunos paquetes de Python"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTXMyfbnAGz9",
        "outputId": "f360b676-886d-41b9-c050-a5ed4620633c"
      },
      "source": [
        "!pip install opencv-python\n",
        "!git clone https://github.com/akcarsten/convert_IDX.git\n",
        "!cd convert_IDX/ \n",
        "!python setup.py install\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (4.1.2.30)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python) (1.19.5)\n",
            "Cloning into 'convert_IDX'...\n",
            "remote: Enumerating objects: 37, done.\u001b[K\n",
            "remote: Total 37 (delta 0), reused 0 (delta 0), pack-reused 37\u001b[K\n",
            "Unpacking objects: 100% (37/37), done.\n",
            "/bin/bash: line 0: cd: /convert_IDX/: No such file or directory\n",
            "python3: can't open file 'setup.py': [Errno 2] No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnXGnmcI_SXg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "6baf2876-0909-4b05-c630-4b2fefc3ce7a"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "#import scipy.misc\n",
        "import imageio\n",
        "from idx_tools import Idx\n",
        "import os\n",
        "\n",
        "%matplotlib inline "
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-287fb6380994>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#import scipy.misc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimageio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0midx_tools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIdx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'idx_tools'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfuvKLjM_SXh"
      },
      "source": [
        "## Extracting letters from an image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DigxoJGJ_SXh"
      },
      "source": [
        "So, the first step will be to extract the individual characters from the images. In order to to make this process more efficient we want to 1) reduce the data, 2) adjust for varying lightning conditions and 3) denoise the result.\n",
        "The following function takes care of the above points:\n",
        "<br>\n",
        "<br>\n",
        "It first reduces the colored image to a gray scale image. So, we move from three color channels to one gray channel.<br>\n",
        "Then the function applies an adaptive thresholding which binarizes the image into black and white values (no gray anymore). While normal thresholding would use one thresholding value for the whole image, adaptive thresholds set different thresholds for small regions of the image. This way we can take care of varying illumination conditions in the images. For details on thresholding have a look [here](https://docs.opencv.org/3.2.0/d7/d4d/tutorial_py_thresholding.html).<br>\n",
        "\n",
        "Finally, the function denoises the thresholded image by applying a blur filter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsuyZbnQ_SXi"
      },
      "source": [
        "def convert_image(img, blur=3):\n",
        "    # Convert to grayscale\n",
        "    conv_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Adaptive thresholding to binarize the image\n",
        "    conv_img = cv2.adaptiveThreshold(conv_img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 4)\n",
        "\n",
        "    # Blur the image to reduce noise\n",
        "    conv_img = cv2.medianBlur(conv_img, blur)\n",
        "    \n",
        "    return conv_img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlkiBXI6_SXi"
      },
      "source": [
        "Now before we continue, let’s have a look what the above functions are doing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NACHE9i_SXj"
      },
      "source": [
        "# Read image\n",
        "img = cv2.imread('./images/01.jpg')\n",
        "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "conv_img = convert_image(img)\n",
        "\n",
        "# Show the effects of each processing stage\n",
        "fig, ax = plt.subplots(1, 3, figsize=(20, 15))\n",
        "\n",
        "cmaps = ['jet', 'gray', 'gray']\n",
        "titles = ['Original', 'Gray scaled', 'Converted']\n",
        "data = [img, gray, conv_img]\n",
        "for i in range(3):\n",
        "    ax[i].imshow(data[i], cmap=cmaps[i])\n",
        "    ax[i].set_title(titles[i], fontsize=23)\n",
        "    ax[i].set_xticks([])\n",
        "    ax[i].set_yticks([])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYBZf4Lx_SXk"
      },
      "source": [
        "After converting our input images, we are ready to extract the individual characters. We will use Open CVs *findContours* function for this as follows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tliBV7s_SXl"
      },
      "source": [
        "def extract_char(conv_img):\n",
        "    # Find contours\n",
        "    ctrs, _ = cv2.findContours(conv_img, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    return ctrs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRixuRG9_SXl"
      },
      "source": [
        "Ok, now what we will get from the above two functions are the x and y coordinates plus the hight and width of all potential letters in an image. But not the images of these letters.<br>\n",
        "\n",
        "Also, what we need to take care of is how to store the single letter images. The function below will take care off all of this by taking a raw input image, converting it to gray scale and then cutting out the letters according to the coordinates from the *extract_char* function. Further it will draw the bounding boxes for each letter onto the raw image so that we can later get an idea of how well the characters were identified.<br>\n",
        "\n",
        "In addition, a folder is created (default name: *char*) in which the images are stored. Also, not all of the contours found in the images will be characters. They can be noise or artefacts so we will implement a simple \"quality gate\" by defining a range of pixels which every image needs to have to be considered as a letter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-Geo26e_SXm"
      },
      "source": [
        "def save_char(ctrs, img, lower=600, upper=20000, path='./char'):\n",
        "    \n",
        "    # Create the target folder for saving the extracted images\n",
        "    if not os.path.isdir(path):\n",
        "        os.mkdir(path)\n",
        "    \n",
        "    # Convert original image to gray scale\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    \n",
        "    # Number of images already in the target folder\n",
        "    n = len(os.listdir('./char')) + 1\n",
        "    \n",
        "    # Number of potential characters found in the image\n",
        "    n_char = np.shape(ctrs)[0]\n",
        "    \n",
        "    # Go through each potential character\n",
        "    for i in range(n_char):\n",
        "        \n",
        "        # Get coordinates of the potential character\n",
        "        x, y, w, h = cv2.boundingRect(ctrs[i])\n",
        "        \n",
        "        # Test if the number of pixels in the bounding box is reasonable\n",
        "        if (w * h) > lower and (w * h) < upper:\n",
        "            \n",
        "            # Draw the bounding box in the original image\n",
        "            result = cv2.rectangle(img, (x, y), ( x + w, y + h ), (0, 255, 0), 2)\n",
        "\n",
        "            # Extract the character and save it as a .jpeg\n",
        "            roi = gray[y:y+h, x:x+w]\n",
        "            imageio.imwrite('{}/char_{}.jpg'.format(path, n), roi)\n",
        "            \n",
        "            # Increase counter\n",
        "            n += 1\n",
        "    \n",
        "    # Return image with all bounding boxes\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K93qMh3G_SXm"
      },
      "source": [
        "So now is the time to put it all together the code below will take four images, find all potential letters, extract them and store their images on disk."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVPpfOIq_SXm"
      },
      "source": [
        "# List of all images to create the first training data \n",
        "image_files = ['./images/01.jpg',\n",
        "          './images/02.jpg',\n",
        "          './images/03.jpg',\n",
        "          './images/04.jpg']\n",
        "\n",
        "# Go through all files and extract the characters\n",
        "for file in image_files:\n",
        "\n",
        "    # Read image\n",
        "    img = cv2.imread(file)\n",
        "\n",
        "    # Convert the image (gray/thresholded/blured)\n",
        "    conv_img = convert_image(img)\n",
        "\n",
        "    # Find and sort the contours\n",
        "    ctrs = extract_char(conv_img)\n",
        "\n",
        "    # Save the result\n",
        "    result = save_char(ctrs, img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-g2mZ_r_SXn"
      },
      "source": [
        "As mentioned above the *save_char* function also draws the bounding boxes onto the raw image. So let's have a look on how the extraction process worked for the last image (*04.jpeg*)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIkkV1TC_SXn"
      },
      "source": [
        "# Plot the result of the last image\n",
        "fig, ax = plt.subplots(1, 1, figsize=(18, 32))\n",
        "\n",
        "ax.imshow(result)\n",
        "ax.set_xlim([1000, 2400])\n",
        "ax.set_ylim([1800, 1420])\n",
        "ax.set_xticks([])\n",
        "ax.set_yticks([])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64QA5fX4_SXo"
      },
      "source": [
        "The result looks quite good; however, we can see some things that might give us a hard time later. For example, at the end of the first line the *i'*s are not detected. Also, the letter *ü* is detected as an *u* and the letter *h* and sometimes *g* is detected as two letters.\n",
        "We will have to see at a later stage how we can fix this. However, for now we can live with it as we want to create a training dataset, not optimize the detection routine."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihRjzh3__SXo"
      },
      "source": [
        "## Clustering the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0dBVq3b_SXo"
      },
      "source": [
        "The next step will be to create our labeled dataset. This means all the individual letters that we extracted in the previous part need to be labeled. Doing this all manually is a tedious task. So, we will try to pre-cluster the data as much as possible. However, we will not get around some manually work at the end."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_UOoNB__SXo"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfo0Z5Qo_SXp"
      },
      "source": [
        "Now, before we start with any clustering, we first need to ensure that all the images have the same size. As we can see from the figure below, currently this is not the case. Each image is the size of its bounding box, which varies over a broad scale. So, in the first step we will resize all images to size of 80x80 pixels and append all of them in a NumPy array.\n",
        "<br>\n",
        "\n",
        "![Overview of extracted images](https://github.com/akcarsten/cook_book/blob/master/images/readme/01.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iZ00Nkz_SXp"
      },
      "source": [
        "# Path to the individual letters\n",
        "data_path = './char/'\n",
        "\n",
        "# Target image size\n",
        "convSize = [80, 80]\n",
        "\n",
        "# File names of all sample images\n",
        "files = [data_path + x for x in os.listdir(data_path)]\n",
        "\n",
        "# Resize images and append to a numpy array\n",
        "images = []\n",
        "for file in files:\n",
        "    img = cv2.imread(file)[:, :, 0]\n",
        "    img = cv2.resize(img, (convSize[0], convSize[1]))\n",
        "    img = img.reshape(convSize[0] * convSize[1])\n",
        "    images.append(img)\n",
        "    \n",
        "images = np.array(images, dtype='float64')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZjG1bov_SXp"
      },
      "source": [
        "Alright now that every image has the same size, we will also have to normalize them for further processing. For this we will use the scikit learn's *StandardScaler*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TntPY2k7_SXp"
      },
      "source": [
        "# Apply StandardScaler on the letter data\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(images)\n",
        "scaled = scaler.transform(images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlrRQCSH_SXq"
      },
      "source": [
        "Next, we will need to reduce the dimensionality of the data. Currently each image contains 80x80 = 6400 pixels or features. This is too much for a clustering algorithm  so we will use Principal Component Analysis (PCA) to reduce the number of dimensions from 6400 to 25. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mFf0Su5_SXq"
      },
      "source": [
        "# Calculate the first 25 principal componnents\n",
        "pca = PCA(n_components=25)\n",
        "pca.fit(scaled)\n",
        "pca_img = pca.transform(scaled)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAL3Nh7q_SXq"
      },
      "source": [
        "Great, now we got everything ready to do some pre-clustering which should help us to bring the dataset from an unlabeled to a labeled state. We will go with K-Means clustering here as it is simple and fast. The only thing that we need to provide is the number of clusters that we expect. If K-Means would give us a perfect result, we would end up with the exact number of letters in the German alphabet. There are 26 letters + 4 special letters (ä,ö,ü, ß) in the German alphabet. Each one of them except ß can appear as a lower or upper case which means in total, we would expect 59 clusters. Additionally, there are 10 numbers (0-9) expected, increasing the number of clusters to 69. Also, there will be artifacts in the data as well as \".\" and \",\" etc. which may have been detected. We will therefore go with 100 clusters. This is more than we need but it will be easier to merge clusters later than to break them apart."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tz3oMmgj_SXq"
      },
      "source": [
        "# Use K-Means clustering to group the data into 100 clusters\n",
        "nClusters = 100\n",
        "kmeans = KMeans(n_clusters=nClusters, random_state=0).fit(pca_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syUyj99w_SXr"
      },
      "source": [
        "Alright, now let's have a brief look at the result. For this we will plot the scores of the first three principal components in 3-D space and color them according to their cluster number. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssyvIoTX_SXr"
      },
      "source": [
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig = plt.figure(figsize=(12, 12))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "\n",
        "ax.scatter(pca_img[:, 0], pca_img[:, 1], pca_img[:, 2], c=kmeans.labels_)\n",
        "\n",
        "ax.set_xlabel('component 1', fontsize=18)\n",
        "ax.set_ylabel('component 2', fontsize=18)\n",
        "ax.set_zlabel('component 3', fontsize=18)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0lQnSXh_SXr"
      },
      "source": [
        "It is quite hard to tell from this plot how well the \"pre-labeling\" worked. We can get a better idea of the clustering result if we move all images into separate folders based on their cluster label. The code below will take care of this and store the results in the folder \"clustered\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvjgCIgw_SXr"
      },
      "source": [
        "path = './clustered'\n",
        "\n",
        "if not os.path.isdir(path):\n",
        "    os.mkdir(path)\n",
        "\n",
        "n = 0\n",
        "for i in range(kmeans.labels_.max()):\n",
        "    \n",
        "    cluster_path = '{}/{}'.format(path, i)\n",
        "    \n",
        "    if not os.path.isdir(cluster_path):\n",
        "        os.mkdir(cluster_path)\n",
        "    \n",
        "    tmp = images[kmeans.labels_ == kmeans.labels_[i]]\n",
        "    \n",
        "    for j in range(np.shape(tmp)[0]):\n",
        "        tmpImg = np.reshape(tmp[j], convSize).astype(np.uint8)\n",
        "        imageio.imwrite('{}/{}.jpg'.format(cluster_path, n), tmpImg)\n",
        "        n += 1\n",
        "        \n",
        "# Delete the un-clustered data\n",
        "[os.remove(data_path + x) for x in os.listdir(data_path)]\n",
        "os.rmdir(data_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5zGtDUb_SXr"
      },
      "source": [
        "By looking into one of the new folders we can see that the clustering worked quite well.\n",
        "<br>\n",
        "![Overview of extracted images](https://github.com/akcarsten/cook_book/blob/master/images/readme/02.png?raw=1)\n",
        "<br>\n",
        "Some other clusters however do not look as clean as the above.\n",
        "<br>\n",
        "![Overview of extracted images](https://github.com/akcarsten/cook_book/blob/master/images/readme/03.png?raw=1)\n",
        "<br>\n",
        "So now unfortunately there is no automated way forward anymore. The next step is to clean up the clusters manually. This means moving the misclassified images into the correct folders and merging folders with identical letters. After this step we also should name the folders according to their content. Below is an example of how it should look like. As you can see there are usually upper- and lower-case letters found in the images. Since we cannot have folders with the same name we will use _ to denote the upper-case letters. Also, there are some cases in which two letters become one. For example, the letter sequence \"ck\" is not separated by any white space and will always be detected by the detection algorithm as one contour so we will create a folder called \"ck\" to account for this. <br>\n",
        "**Note: We will create all of this folders within a folder called \"data\".**\n",
        "<br>\n",
        "![Overview of extracted images](https://github.com/akcarsten/cook_book/blob/master/images/readme/04.png?raw=1)\n",
        "<br>\n",
        "So, the final step to wrap all of this up is to convert the dataset into the IDX format. You might be familiar with this format already as the famous MNIST dataset is saved in the same way. Only in this case instead of reading the data from an IDX file we must write it.\n",
        "<br>\n",
        "We will do so by using the [idx_converter](https://github.com/akcarsten/convert_IDX) which takes a file structure as we set it up in this notebook and directly saves it in the IDX format. The output will be two files: one file with the images and a second file with the labels.\n",
        "<br>\n",
        "Since we want to later train a classifier on the data, we should already split the images into a training and a test dataset. For this we will move 30% of the images in each category to a test folder. The code below takes care of all of this. All of the below folders will be within a folder called \"data\" within this repository. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hQEeDHr_SXs"
      },
      "source": [
        "# Where is the sorted data located?\n",
        "dataset_path = './data/' \n",
        "\n",
        "# Where should the converted data be stored?\n",
        "dest_folder = './datasetTMP/' \n",
        "\n",
        "# What is the ratio of train vs. test data?\n",
        "train_percent = 0.7\n",
        "\n",
        "# Subfolders for structuring the sorted data\n",
        "train_path = dataset_path + 'train/'\n",
        "test_path = dataset_path + 'test/'\n",
        "\n",
        "# Find all folders in the sorted data\n",
        "folders = os.listdir(dataset_path)\n",
        "\n",
        "# Create folders\n",
        "if not os.path.isdir(train_path):\n",
        "    os.mkdir(train_path)\n",
        "    os.mkdir(test_path)\n",
        "    \n",
        "if not os.path.isdir(dest_folder):\n",
        "    os.mkdir(dest_folder)\n",
        "    \n",
        "# Go through all folders in the sorted data and split into train and test    \n",
        "for char in folders:\n",
        "    char_path = dataset_path + char + '/'\n",
        "    train_folder = train_path + char + '/'\n",
        "    test_folder = test_path + char + '/'\n",
        "    \n",
        "    samples = os.listdir(char_path)\n",
        "    n_samples = len(samples)\n",
        "    n_train = round(train_percent * n_samples)\n",
        "    \n",
        "    sel = np.arange(n_samples)\n",
        "    np.random.shuffle(sel)\n",
        "    \n",
        "    idx_train = sel[0:n_train] \n",
        "    idx_test = sel[n_train:]\n",
        "    \n",
        "    if not os.path.isdir(train_folder):\n",
        "        os.mkdir(train_folder)\n",
        "        \n",
        "    if not os.path.isdir(test_folder):\n",
        "        os.mkdir(test_folder)\n",
        "    \n",
        "    [os.rename(char_path + samples[x], train_folder + samples[x]) for x in idx_train]\n",
        "    [os.rename(char_path + samples[x], test_folder + samples[x]) for x in idx_test]\n",
        "    \n",
        "    os.rmdir(char_path)\n",
        "    \n",
        "# Convert data to idx format    \n",
        "Idx.save_idx(train_path)\n",
        "Idx.save_idx(test_path)\n",
        "\n",
        "# Move converted dataset to target folder\n",
        "os.rename(train_path + 'images.idx3-ubyte', dest_folder + 'train-images.idx3-ubyte')\n",
        "os.rename(train_path + 'labels.idx3-ubyte', dest_folder + 'train-labels.idx3-ubyte')\n",
        "os.rename(test_path + 'images.idx3-ubyte', dest_folder + 'test-images.idx3-ubyte')\n",
        "os.rename(test_path + 'labels.idx3-ubyte', dest_folder + 'test-labels.idx3-ubyte')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7E9SB53M_SXs"
      },
      "source": [
        "To make our lives easier later we will also create a txt file with the names of the characters in the dataset. As we discussed above upper-case letters were located in a folder ending with \"_\". We will take this into account and convert these letters to upper case in the txt file. The code below takes care of all of this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugDlJzJx_SXs"
      },
      "source": [
        "chars = os.listdir('./data/train')\n",
        "\n",
        "labelFile = open('{}/labels.txt'.format(dest_folder), \"w\")\n",
        "for char in chars:\n",
        "    \n",
        "    if char.endswith('_'):\n",
        "        char = char[:-1].upper()\n",
        "    \n",
        "    # write line to output file\n",
        "    labelFile.write(char)\n",
        "    labelFile.write(\"\\n\")\n",
        "labelFile.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}