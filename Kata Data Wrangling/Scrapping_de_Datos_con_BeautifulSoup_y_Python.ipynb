{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "Scrapping de Datos con BeautifulSoup y Python.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6H87vXjLNYR"
      },
      "source": [
        "# Tutorial de Web Scraping usando BeautifulSoup y Python\n",
        "Aprende a hacer web scraping de datos usando la biblioteca BeautifulSoup\n",
        "\n",
        "## Acerca de BeautifulSoup\n",
        "BeautifulSoup nos facilita analizar gramáticamente datos útiles de un sitio web construido en HTML.\n",
        "Para poder usar esta biblioteca, tenemos primero que\n",
        "instalarla en nuestro ambiente local utilizando la terminal y ejecutando el siguiente comando: pip install beautifulsoup4\n",
        "\n",
        "Puedes echar un vistazo a la [documentación oficial de BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) para ver el conjunto completo de funciones.\n",
        "\n",
        "### Importamos la biblioteca requests para obtener el contenido HTML.\n",
        "Puedes ver que nuestro ejemplo contiene cerca de 42k caracteres."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tz5SRFi0LNYU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d50021d9-fe14-4e6e-e02a-62cdeb365cfd"
      },
      "source": [
        "import requests\n",
        "r = requests.get('https://www.usclimatedata.com/climate/united-states/us')\n",
        "print(len(r.text))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "42402\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKTC_1wALNYV"
      },
      "source": [
        "### Importamos BeautifulSoup y convertimos el HTML en un objeto bs4 \n",
        "\n",
        "Ahora podemos acceder a etiquetas específicas de HTML en la página usando el punto (.), justo como si fuera un JSON Object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EAoIddOLNYW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "deb08e1e-3f1c-4f5e-d478-559012ad1291"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "soup = BeautifulSoup(r.text)\n",
        "print(soup.title)\n",
        "print(soup.title.string)\n",
        "\n",
        "\n",
        "#############################################\n",
        "# Reto 0. Utiliza otra página web , observa su árbol de etiquetas desde Chrome \n",
        "# Dev Tools y usa la documentación de bs4 para imprimir 3 etiquetas más.\n",
        "#############################################\n"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<title>Climate United States - Normals and averages</title>\n",
            "Climate United States - Normals and averages\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4W4GufxLNYW"
      },
      "source": [
        "\n",
        "### Profundizando en el objeto bs4 para acceder a contenido del sitio web.\n",
        "\n",
        "soup.p nos dará el contenido de la primera etiqueta de tipo párrafo en la página web.\n",
        "\n",
        "soup.a nos da los hipervínculos del sitio.\n",
        "\n",
        "Podemos obtener los contenidos de un atributo dentro de una etiqueta HTML usando los brackets cuadrados y paréntesis.\n",
        "\n",
        "Usamos .parent para obtener el objeto padre, y .next_sibling para obtener el objeto en la misma dimension.\n",
        "\n",
        "**Podemos usar Chrome Dev Tools para inspeccionar elementos y encontrar la etiqueta que necesitemos**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lj8RS-VQLNYW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ca8d031-077c-4917-c4f7-9646104fb5aa"
      },
      "source": [
        "print(soup.p)\n",
        "print(soup.p.text)\n",
        "print(soup.a)\n",
        "print(soup.a['title'])\n",
        "print()\n",
        "print(soup.p.parent)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<p class=\"selection_title\">Select a state by name</p>\n",
            "Select a state by name\n",
            "<a class=\"navbar-brand\" href=\"/\" title=\"Temperature - Precipitation - Sunshine - Snowfall\"><img alt=\"Temperature - Precipitation - Sunshine - Snowfall\" data-src=\"https://www.usclimatedata.com/assets/images/us-climate-data.png\" height=\"34\" src=\"https://www.usclimatedata.com/assets/images/us-climate-data.png\" srcset=\"https://www.usclimatedata.com/assets/images/us-climate-data.png 1x, https://www.usclimatedata.com/assets/images/us-climate-data-2.png 2x\" width=\"31\"/><span class=\"white ml-2\">U.S. Climate Data</span></a>\n",
            "Temperature - Precipitation - Sunshine - Snowfall\n",
            "\n",
            "<div class=\"float-left mb-4 mt-2\"><p class=\"selection_title\">Select a state by name</p></div>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96eNukEjLNYX"
      },
      "source": [
        "### Prettify() es muy útil para formateo de texto\n",
        "\n",
        "Este método es genial pero pero obseremos que sólo funciona con objetos bs4, no en caracteres, diccionarios o listas. Para esos objetos necesitamos importar pprint.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5T_aopHLNYX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fca1067-0ff8-4662-9a1c-b60cfd3e0406"
      },
      "source": [
        "print(soup.p.parent.prettify())"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<div class=\"float-left mb-4 mt-2\">\n",
            " <p class=\"selection_title\">\n",
            "  Select a state by name\n",
            " </p>\n",
            "</div>\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXjU-TYWLNYX"
      },
      "source": [
        "### Necesitamos todos los links de cada estado en esta página\n",
        "Primero necesitamos encontrar todos los hipervínculos, para imprimir únicamente el atributo href, que es el link realmente.\n",
        "\n",
        "Pero podemos observar que el resultado incluye algunos hipervínculos que no necesitamos, entonces necesitamos filtrar y remover esos.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dkVmzSPLNYY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc050a8c-0bde-4cce-b044-28638e2cb966"
      },
      "source": [
        "for link in soup.find_all('a'):\n",
        "    print(link.get('href'))"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/\n",
            "#\n",
            "/\n",
            "/climate/united-states/us\n",
            "/\n",
            "/climate/united-states/us\n",
            "/climate/alabama/united-states/3170\n",
            "/climate/alaska/united-states/3171\n",
            "/climate/arizona/united-states/3172\n",
            "/climate/arkansas/united-states/3173\n",
            "/climate/california/united-states/3174\n",
            "/climate/colorado/united-states/3175\n",
            "/climate/connecticut/united-states/3176\n",
            "/climate/delaware/united-states/3177\n",
            "/climate/district-of-columbia/united-states/3178\n",
            "/climate/florida/united-states/3179\n",
            "/climate/georgia/united-states/3180\n",
            "/climate/hawaii/united-states/3181\n",
            "/climate/idaho/united-states/3182\n",
            "/climate/illinois/united-states/3183\n",
            "/climate/indiana/united-states/3184\n",
            "/climate/iowa/united-states/3185\n",
            "/climate/kansas/united-states/3186\n",
            "/climate/kentucky/united-states/3187\n",
            "/climate/louisiana/united-states/3188\n",
            "/climate/maine/united-states/3189\n",
            "/climate/maryland/united-states/1872\n",
            "/climate/massachusetts/united-states/3191\n",
            "/climate/michigan/united-states/3192\n",
            "/climate/minnesota/united-states/3193\n",
            "/climate/mississippi/united-states/3194\n",
            "/climate/missouri/united-states/3195\n",
            "/climate/montana/united-states/919\n",
            "/climate/nebraska/united-states/3197\n",
            "/climate/nevada/united-states/3198\n",
            "/climate/new-hampshire/united-states/3199\n",
            "/climate/new-jersey/united-states/3200\n",
            "/climate/new-mexico/united-states/3201\n",
            "/climate/new-york/united-states/3202\n",
            "/climate/north-carolina/united-states/3203\n",
            "/climate/north-dakota/united-states/3204\n",
            "/climate/ohio/united-states/3205\n",
            "/climate/oklahoma/united-states/3206\n",
            "/climate/oregon/united-states/3207\n",
            "/climate/pennsylvania/united-states/3208\n",
            "/climate/puerto-rico/united-states/7335\n",
            "/climate/rhode-island/united-states/3209\n",
            "/climate/south-carolina/united-states/3210\n",
            "/climate/south-dakota/united-states/3211\n",
            "/climate/tennessee/united-states/3212\n",
            "/climate/texas/united-states/3213\n",
            "/climate/utah/united-states/3214\n",
            "/climate/vermont/united-states/3215\n",
            "/climate/virginia/united-states/3216\n",
            "/climate/washington/united-states/3217\n",
            "/climate/west-virginia/united-states/3218\n",
            "/climate/wisconsin/united-states/3219\n",
            "/climate/wyoming/united-states/3220\n",
            "/climate/washington/district-of-columbia/united-states/usdc0001\n",
            " https://facebook.com/sharer/sharer.php?u=https://www.usclimatedata.com/climate/united-states/us\n",
            "https://twitter.com/intent/tweet/?text=Climate+United+States+-+Normals+and+averages&url=https://www.usclimatedata.com/climate/united-states/us\n",
            "https://pinterest.com/pin/create/button/?url=https://www.usclimatedata.com/climate/united-states/us&media=description=Climate+United+States+-+Normals+and+averages\n",
            "mailto:mail@example.com?subject=Climate+United+States+-+Normals+and+averages&body=Climate+United+States+-+Normals+and+averages+-+https%3A%2F%2Fwww.usclimatedata.com%2Fclimate%2Funited-states%2Fus\n",
            "https://www.tumblr.com/widgets/share/tool?posttype=link&title=Climate+United+States+-+Normals+and+averages&caption=&content=Climate+United+States+-+Normals+and+averages+-+https%3A%2F%2Fwww.usclimatedata.com%2Fclimate%2Funited-states%2Fus&canonicalUrl=https://www.usclimatedata.com/climate/united-states/us&shareSource=tumblr_share_button\n",
            "whatsapp://send?text=Climate+United+States+-+Normals+and+averages+-+https%3A%2F%2Fwww.usclimatedata.com%2Fclimate%2Funited-states%2Fus\n",
            "https://www.facebook.com/yourweatherservice\n",
            "https://twitter.com/usclimatedata\n",
            "/website-info\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RM317zkdLNYY"
      },
      "source": [
        "### Filter urls using string functions\n",
        "### Filtrando URLs usando funciones de caracteres\n",
        "Con un mágico **if** para verificar condiciones, y luego agregamos los que nos importan a una lista. \n",
        "Al final, obtendremos 51 hipervínculos de estado, incluyendo Washington DC.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giSv8FCcLNYY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95319a5d-e731-4303-f87b-b100098632e6"
      },
      "source": [
        "base_url = 'https://www.usclimatedata.com'\n",
        "state_links = []\n",
        "for link in soup.find_all('a'):\n",
        "    url = link.get('href')\n",
        "    if url and '/climate/' in url and '/climate/united-states/us' not in url:\n",
        "        state_links.append(url)\n",
        "print(len(state_links))"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "53\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4VA9ROULNYZ"
      },
      "source": [
        "### Hagamos una prueba obteniendo datos de sólo un estado\n",
        "Y luego imprimimos el título de esa página web."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "u-qhrr6dLNYZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "281acc5c-98de-4145-b540-32b501b369fd"
      },
      "source": [
        "r = requests.get(base_url + state_links[5])\n",
        "soup = BeautifulSoup(r.text)\n",
        "print(soup.title.string)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Climate Colorado - Temperature, Rainfall and Averages\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HrJ6Z0uLNYZ"
      },
      "source": [
        "### Los datos que necesitamos están en la etiqueta *tr*\n",
        "Pero mira, hay 14 etiquetas de tipo tr en la página, y sólo queremos 2 de ellas (las columnas de **average high**)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhlDTb6zLNYZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cfad5f4-36f8-4906-bbe5-13b7a86bb4c4"
      },
      "source": [
        "rows = soup.find_all('tr')\n",
        "print(len(rows))"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bno_xBpkLNYZ"
      },
      "source": [
        "### Filtrando filas, y agregando datos de temperatura a la lista.\n",
        "Usaremos **List comprehension** para filtrar las filas.\n",
        "Entonces ahora tendremos sólo 2 filas.\n",
        "Iteramos esas 2 filasm y agregamos todas las temperaturas para las celdas de datos (td) en la lista.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtWDoU3_LNYa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "090c7f8e-9ad1-4fd5-d723-31f2f7c290e3"
      },
      "source": [
        "rows = [row for row in rows if 'Average high' in str(row)]\n",
        "print(len(rows))\n",
        "\n",
        "high_temps = []\n",
        "for row in rows:\n",
        "    tds = row.find_all('td')\n",
        "    for i in range(1,2):\n",
        "        high_temps.append(tds[i].text)\n",
        "print(high_temps)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n",
            "['58', '91']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plW_6nURLNYa"
      },
      "source": [
        "### Obteniendo el nombre del estado\n",
        "Lo que haremos ahora es primero partir la cadena de caracteres en una lista, y tomamos la segunda palabra.\n",
        "Pero eso no funcionaría para estados con 2 palabras como New York o Carolina del Norte.\n",
        "Entonces vamos a usar una función de cadenas de caracteres para encontrar y cortar una cadena de caracteres desde el guión (-)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZdbOokNLNYa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d9f10ff-e2b7-425f-ff06-eb50c18a17a8"
      },
      "source": [
        "state = soup.title.string.split()[1]\n",
        "print(state)\n",
        "s = soup.title.string\n",
        "state = s[s.find(' '):s.find('-')].strip()\n",
        "print(state)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Alabama\n",
            "Alabama\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ml9XiDmoLNYb"
      },
      "source": [
        "### Add state name and temp list to the data dictionary\n",
        "### Agregamos el nombre del estado y una lista temporal al diccionario de datos.\n",
        "\n",
        "Para un estado, así es como se ven nuestros datos con la técnica de scrapping. \n",
        "En este ejemplo sólo obtuvimos temperaturas máximas por estado, pero podríamos obtener cada ciuidad, o también obtener otros datos relevantes como las temperaturas mínimas y precipicitación.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABfuwYA5LNYb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c72bd8c3-7fa5-434a-c868-8e57d521a0ea"
      },
      "source": [
        "data = {}\n",
        "data[state] = high_temps\n",
        "print(data)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'Alabama': ['58', '91']}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQ0r2BYxLNYb"
      },
      "source": [
        "### Put it all together and iterate 51 states\n",
        "### Juntando todo e iterando en 51 estados.\n",
        "Acá vamos a crear un ciclo en nuestra lista de 51 estados, y obtener la temperatura más alta para cada estado, y agregarla al diccionario de datos.\n",
        "\n",
        "Esto combina todo nuestro trabajo anterior en un sólo ciclo.\n",
        "El resultado es un diccionario con 51 estados y una lista de temperaturas máximas para cada uno.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Pf98PoqLNYb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49014873-371e-4bdd-ba7b-34354b5a2730"
      },
      "source": [
        "data = {}\n",
        "for state_link in state_links:\n",
        "    url = base_url + state_link\n",
        "    r = requests.get(base_url + state_link)\n",
        "    soup = BeautifulSoup(r.text)\n",
        "    rows = soup.find_all('tr')\n",
        "    rows = [row for row in rows if 'Average high' in str(row)]\n",
        "    high_temps = []\n",
        "    for row in rows:\n",
        "        tds = row.find_all('td')\n",
        "        for i in range(1,2):\n",
        "            high_temps.append(tds[i].text)\n",
        "    s = soup.title.string\n",
        "    state = s[s.find(' '):s.find('-')].strip()\n",
        "    data[state] = high_temps\n",
        "print(data)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'Alabama': ['58', '91'], 'Alaska': ['27', '64'], 'Arizona': ['71', '104'], 'Arkansas': ['55', '93'], 'California': ['60', '91'], 'Colorado': ['46', '88'], 'Connecticut': ['40', '81'], 'Delaware': ['47', '85'], 'District Of Columbia': ['44', '84'], 'Florida': ['67', '92'], 'Georgia': ['57', '88'], 'Hawaii': ['80', '89'], 'Idaho': ['45', '90'], 'Illinois': ['36', '82'], 'Indiana': ['40', '83'], 'Iowa': ['36', '84'], 'Kansas': ['45', '89'], 'Kentucky': ['45', '86'], 'Louisiana': ['65', '91'], 'Maine': ['32', '78'], 'Maryland': ['46', '87'], 'Massachusetts': ['39', '80'], 'Michigan': ['33', '80'], 'Minnesota': ['31', '82'], 'Mississippi': ['60', '92'], 'Missouri': ['45', '88'], 'Montana': ['39', '85'], 'Nebraska': ['37', '86'], 'Nevada': ['50', '88'], 'New Hampshire': ['35', '81'], 'New Jersey': ['42', '84'], 'New Mexico': ['48', '83'], 'New York': ['42', '83'], 'North Carolina': ['55', '87'], 'North Dakota': ['28', '83'], 'Ohio': ['40', '84'], 'Oklahoma': ['55', '93'], 'Oregon': ['52', '82'], 'Pennsylvania': ['44', '85'], 'Puerto Rico': ['83', '88'], 'Rhode Island': ['40', '81'], 'South Carolina': ['63', '89'], 'South Dakota': ['27', '82'], 'Tennessee': ['55', '91'], 'Texas': ['65', '97'], 'Utah': ['44', '89'], 'Vermont': ['31', '79'], 'Virginia': ['51', '88'], 'Washington': ['44', '84'], 'West Virginia': ['47', '84'], 'Wisconsin': ['33', '78'], 'Wyoming': ['40', '81']}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btrKdYxfLNYc"
      },
      "source": [
        "### Save to CSV file\n",
        "### Guardando todo en un archivo CSV.\n",
        "Finalmente, podría interesarnos escribir todo en un archivo CSV.\n",
        "Acá tenemos una manera fácil para lograr eso."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VVrKLzaLNYc"
      },
      "source": [
        "import csv\n",
        "\n",
        "with open('high_temps.csv','w') as f:\n",
        "    w = csv.writer(f)\n",
        "    w.writerows(data.items())"
      ],
      "execution_count": 84,
      "outputs": []
    }
  ]
}