{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data_wrangling_cheatsheet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgrWy48OJ9uF"
      },
      "source": [
        "Importamos Pandas y Numpy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewsFUAn8JxfG"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzdeXLW6J5BG"
      },
      "source": [
        "Then comes connecting to working directories.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_f-PfRtKJ1Qh"
      },
      "source": [
        "import os\n",
        "# show current working directory\n",
        "os.getcwd()\n",
        "# list files in the directory\n",
        "os.listdir()\n",
        "# change working directory\n",
        "os.chdir(\"/\")"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sO6kvbDFKICK"
      },
      "source": [
        ""
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-3uNQ-OKdff"
      },
      "source": [
        "#2. Loading data\n",
        "Now load in your datasets from the repository (desktop, cloud, SQL server — wherever stored). It’s a good idea to make a copy of the original dataset and work with the copy because you’ll be doing a lot of modifications to the original one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "id": "1ahSAl_NKdT3",
        "outputId": "31a5c28d-7826-46d7-fece-d17912321ba1"
      },
      "source": [
        "# import from a csv file\n",
        "#data = pd.read_csv(\"Iris.csv\")\n",
        "data = pd.read_csv(\"data.csv\")\n",
        "# copying a dataset\n",
        "df = data.copy()\n",
        "# call the head function\n",
        "df.head()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-8b87252b9e3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# import from a csv file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#data = pd.read_csv(\"Iris.csv\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# copying a dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XlRPHBJMsdG"
      },
      "source": [
        "#3. Initial data screening\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42TkcOaeMs9p"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHCMoiukMyvr"
      },
      "source": [
        "# number of rows and columns\n",
        "df.shape\n",
        "# column names\n",
        "df.columns\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYVyCO1YM8J9"
      },
      "source": [
        "# number of unique values\n",
        "df[\"species\"].nunique()\n",
        "# name of the unique values\n",
        "df[\"species\"].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwYnrn2BM8De"
      },
      "source": [
        "# count of categorical data\n",
        "df[\"species\"].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOtIrBbUNCcL"
      },
      "source": [
        "#4. Missing value treatment\n",
        "Missing values are no surprise. The yellow highlighted cells in the dataframe above are NaN values. You can search for the number of missing values in a dataset by typing the following:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9e-cjZ-M7ig"
      },
      "source": [
        "# show NaN values per feature\n",
        "df.isnull().sum()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0--m0FM5Oqvo"
      },
      "source": [
        "You could also obtain missing values as a percentage of total observations (it is quite useful for large datasets).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQz6J0TQOs9_"
      },
      "source": [
        "# NaN values as % of total observations\n",
        "df.isnull().sum()*100/len(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObIGAXBNOsv3"
      },
      "source": [
        "In the iris dataset we have two missing values in the sepal_length column. Now that we found out, what to do with them? You can do one of the following:\n",
        "\n",
        "a ) drop the rows or columns containing null values;\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66JQQNyYO1mc"
      },
      "source": [
        "## Drop row/column ##\n",
        "#####################\n",
        "# drop all rows containing null\n",
        "df.dropna()\n",
        "# drop all columns containing null\n",
        "df.dropna(axis=1)\n",
        "# drop columns with less than 5 NaN values\n",
        "df.dropna(axis=1, thresh=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOp7y7WCO_IA"
      },
      "source": [
        "b ) or, replace/impute missing cells with some other values;\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dKAvjFcPBvZ"
      },
      "source": [
        "## Replace values ##\n",
        "####################\n",
        "# replace all na values with -9999\n",
        "df.fillna(-9999)\n",
        "[# additional tip: you can also replace any specific cell values\n",
        "df.at[1, \"sepal_length\"]= 9999]\n",
        "# fill na values with NaN\n",
        "df.fillna(np.NaN)\n",
        "# fill na values with strings\n",
        "df.fillna(\"data missing\")\n",
        "# fill missing values with mean column values\n",
        "df.fillna(df.mean())\n",
        "# replace na values of specific columns with mean value\n",
        "df[\"sepal_length\"].fillna(df[\"sepal_length\"].mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkuOkESMPBY3"
      },
      "source": [
        "c ) or, if it’s time-series data, interpolation is a great way to impute data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyFwqg4-PJwG"
      },
      "source": [
        "## Interpolate ##\n",
        "#################\n",
        "# interpolation of missing values (useful in time-series)\n",
        "df.interpolate() # all dataframe\n",
        "df[\"sepal_length\"].interpolate() # specific column"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOjTopeHPKrT"
      },
      "source": [
        "#5. Subsetting & working with columns\n",
        "Not all columns in the dataset are of interest, sometimes we select specific columns for analytics or building a model. Subsetting allows you to do that.\n",
        "There are two key ways to select columns: by column names and by column positions:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldGAmAAdPOs-"
      },
      "source": [
        "# select a column by column name\n",
        "df[\"sepal_length\"]\n",
        "# select multiple columns by column name\n",
        "df[[\"sepal_length\", \"sepal_width\", \"petal_length\", \"spp\"]]\n",
        "# select a column by column number\n",
        "df.iloc[:, 2:4]\n",
        "# select multiple columns by column number\n",
        "df.iloc[:, [1,3,4]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AbGSHv1PREs"
      },
      "source": [
        "But what if you want to subset data by dropping a column?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XuSaQmAPS3X"
      },
      "source": [
        "# drop a column \n",
        "df.drop(\"sepal_length\", axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuHv9CUDPVRV"
      },
      "source": [
        "Now let’s say you want to create a new column by adding two existing columns:\n",
        "sepal_len_cm= sepal_length* 10\n",
        "Creating new calculated columns is often big part of feature engineering."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yB28ODRKPZqt"
      },
      "source": [
        "# add new calculated column\n",
        "df['new'] = df[\"sepal_length\"]*2\n",
        "# create a conditional calculated column\n",
        "df['newcol'] = [\"short\" if i<3 else \"long\" for i in df[\"sepal_width\"]] \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrE4jjuiPd0T"
      },
      "source": [
        "Sometimes re-coding may be needed to convert categorical string values to numeric values.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkZkFaX5Pecm"
      },
      "source": [
        "df.replace({\"Species\":{\"setosa\":1, \"versicolor\":2, \"virginica\":3}})\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SeJKrrVPgNQ"
      },
      "source": [
        "If aggregation of column values is needed (mean/median etc.), python and numpy has native functions that can be applied to the dataframe.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fg1P7hXfPm4U"
      },
      "source": [
        "# calculate mean of each of two columns\n",
        "df[[\"sepal_length\", \"sepal_width\"]].mean()\n",
        "# calculate sum and mean of each column\n",
        "df[[\"sepal_length\", \"sepal_width\"]].agg([np.sum, np.mean])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6Z4QzOCPmmV"
      },
      "source": [
        "And finally, some bonus syntax useful to work with columns:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xFG8eqaPvLu"
      },
      "source": [
        "# transposing a dataset\n",
        "df.T\n",
        "# create a list of columns\n",
        "df.columns.tolist()\n",
        "# sorting values in ascending order\n",
        "df.sort_values(by = \"sepal_width\", ascending = True)\n",
        "# change column name\n",
        "df.rename(columns={\"old_name\": \"new_name\"})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ha5kdcvUPgBU"
      },
      "source": [
        "#6. Filtering: working with rows\n",
        "\n",
        "Filtering is an important part of exploratory data analysis, drawing insights and building KPIs.\n",
        "There are many ways to filter data depending on the analytics needs, such as:\n",
        "a) using the row index location:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvmcT70fPfsM"
      },
      "source": [
        "# select rows with index number 3 to 10\n",
        "df.iloc[3:10,]\n",
        "# select rows with index name\n",
        "df.loc[\"index1\", \"index2\"]\n",
        "# finding rows with specific strings\n",
        "df[df[\"species\"].isin([\"setosa\"])]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTHBHjlZP3gD"
      },
      "source": [
        "b) conditional filtering\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9G5VvuhbP3MF"
      },
      "source": [
        "# simple conditional filtering to filter rows with sepal_length>=5\n",
        "df.query('sepal_length>=5') # or\n",
        "df[df.sepal_length>= 5]\n",
        "# filtering rows with multiple values e.g. 0.2, 0.3\n",
        "df[df[\"petal_length\"].isin([0.2, 0.3])]\n",
        "# multi-conditional filtering\n",
        "df[(df.sepal_length>1) & (df.species==\"setosa\") | (df.sepal_width<3)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zL8pgT0oP7y7"
      },
      "source": [
        "And finally, here’s how you’d get rid of a row if needed.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlmTUc2iP-JH"
      },
      "source": [
        "# drop rows\n",
        "df.drop(df.index[1]) # 1 is row index to be deleted\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CjcuIDfQBMi"
      },
      "source": [
        "#7. Grouping\n",
        "\n",
        "Like filtering, grouping is another important part of exploratory data analysis and data visualization. The key function for this task is groupby() and is mainly used for aggregating rows based on categorical features.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_man4W_QAve"
      },
      "source": [
        "# return a dataframe object grouped by \"species\" column\n",
        "df.groupby(\"species\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0SAhariQH8N"
      },
      "source": [
        "After the dataframe is grouped, you could apply different functions to it, for example, getting aggregate values of numeric columns:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oK3sWM70QKXZ"
      },
      "source": [
        "# return mean a column groupby \"species\" categories\n",
        "df[\"sepal_length\"].groupby(df[\"species\"]).mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4gBRX0EQOGL"
      },
      "source": [
        "Or you can apply such aggregate function to multiple features:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tA7RI4ivQN4z"
      },
      "source": [
        "# group each column by \"species\", then apply multiple operation on each feature \n",
        "df.groupby(\"species\").agg([np.sum, np.mean, np.std])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BB8F_J40QR2T"
      },
      "source": [
        "#8. Joining/merging\n",
        "\n",
        "If you know SQL I don’t have to explain how important joining is. Python and pandas have some functions such as merge(), join(), concat() for SQL style joining. If SQL is the primary database you probably won’t have to do joining much in Python, but nevertheless you should add the following codes to your cheatsheet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FiLuuBLjQQ-I"
      },
      "source": [
        "# SQL style joining\n",
        "df1 = df[[\"sepal_length\", \"sepal_width\"]]\n",
        "df2 = df[[\"sepal_length\", \"petal_length\"]]\n",
        "dfx = pd.concat([df1, df2], axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}